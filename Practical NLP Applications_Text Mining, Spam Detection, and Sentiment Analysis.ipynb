{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical NLP Applications: Text Mining, Spam Detection, and Sentiment Analysis**"
      ],
      "metadata": {
        "id": "4hSazgYp64eu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project 1: Preprocessing Demo**"
      ],
      "metadata": {
        "id": "GrCZu3fN7GBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# List of required NLTK resources\n",
        "resources = ['punkt', 'stopwords', 'wordnet']\n",
        "\n",
        "for resource in resources:\n",
        "    try:\n",
        "        nltk.data.find(f'{resource}')\n",
        "    except LookupError:\n",
        "        nltk.download(resource)\n",
        "\n",
        "\n",
        "# Example text\n",
        "text = \"Our customer service received 3 complaints today. However, 25 users gave positive feedback!\"\n",
        "\n",
        "# 1. Sentence & Word Tokenization\n",
        "print(sent_tokenize(text))\n",
        "print(word_tokenize(text))\n",
        "\n",
        "# 2. Remove Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered = [w for w in word_tokenize(text) if w.lower() not in stop_words]\n",
        "print(\"Without stopwords:\", filtered)\n",
        "\n",
        "# 3. Remove Punctuation\n",
        "no_punct = [w for w in filtered if w not in string.punctuation]\n",
        "print(\"Without punctuation:\", no_punct)\n",
        "\n",
        "# 4. Remove Numbers\n",
        "no_numbers = [w for w in no_punct if not w.isdigit()]\n",
        "print(\"Without numbers:\", no_numbers)\n",
        "\n",
        "# 5. Stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "stemmed = [stemmer.stem(w) for w in no_numbers]\n",
        "print(\"Stemmed:\", stemmed)\n",
        "\n",
        "# 6. Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized = [lemmatizer.lemmatize(w) for w in no_numbers]\n",
        "print(\"Lemmatized:\", lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShEaPzn-7DWa",
        "outputId": "62970d05-a597-4ff3-fe15-8e707a41bda9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Our customer service received 3 complaints today.', 'However, 25 users gave positive feedback!']\n",
            "['Our', 'customer', 'service', 'received', '3', 'complaints', 'today', '.', 'However', ',', '25', 'users', 'gave', 'positive', 'feedback', '!']\n",
            "Without stopwords: ['customer', 'service', 'received', '3', 'complaints', 'today', '.', 'However', ',', '25', 'users', 'gave', 'positive', 'feedback', '!']\n",
            "Without punctuation: ['customer', 'service', 'received', '3', 'complaints', 'today', 'However', '25', 'users', 'gave', 'positive', 'feedback']\n",
            "Without numbers: ['customer', 'service', 'received', 'complaints', 'today', 'However', 'users', 'gave', 'positive', 'feedback']\n",
            "Stemmed: ['custom', 'servic', 'receiv', 'complaint', 'today', 'howev', 'user', 'gave', 'posit', 'feedback']\n",
            "Lemmatized: ['customer', 'service', 'received', 'complaint', 'today', 'However', 'user', 'gave', 'positive', 'feedback']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PK0vB0rk84vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project 2: Clustering Demo (Unsupervised)**"
      ],
      "metadata": {
        "id": "tmPbIMZF8mUA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Documents\n",
        "docs = [\n",
        "    \"The sales team achieved a record profit this quarter.\",\n",
        "    \"Customer satisfaction has increased after the new support system.\",\n",
        "    \"The new AI tool is improving data processing efficiency.\",\n",
        "    \"Marketing campaigns boosted online engagement by 30 percent.\",\n",
        "    \"We received excellent feedback from enterprise clients.\"\n",
        "]\n",
        "\n",
        "# Convert text to TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(docs)\n",
        "\n",
        "# Cluster with KMeans\n",
        "k = 2\n",
        "model = KMeans(n_clusters=k, random_state=42)\n",
        "model.fit(X)\n",
        "\n",
        "# Show cluster keywords\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "for i in range(k):\n",
        "    print(\"Cluster\", i)\n",
        "    for idx in order_centroids[i, :10]:\n",
        "        print(\" \", terms[idx])\n",
        "    print()\n",
        "\n",
        "# Predict cluster of new text\n",
        "new_text = [\"chrome browser to open\"]\n",
        "y = vectorizer.transform(new_text)\n",
        "print(\"Prediction:\", model.predict(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PUrQ3r77EXm",
        "outputId": "58d605ca-d037-41a3-b341-1905a3d52f7f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0\n",
            "  new\n",
            "  satisfaction\n",
            "  support\n",
            "  customer\n",
            "  increased\n",
            "  profit\n",
            "  record\n",
            "  quarter\n",
            "  team\n",
            "  sales\n",
            "\n",
            "Cluster 1\n",
            "  received\n",
            "  clients\n",
            "  enterprise\n",
            "  excellent\n",
            "  feedback\n",
            "  sales\n",
            "  record\n",
            "  quarter\n",
            "  profit\n",
            "  team\n",
            "\n",
            "Prediction: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PbC9LNnc9LyF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project 3: SMS Spam Classification (Supervised)**"
      ],
      "metadata": {
        "id": "mPpVn8ik9Nar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "import string, nltk\n",
        "\n",
        "# Load dataset\n",
        "url = \"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\"\n",
        "data = pd.read_csv(url, encoding=\"ISO-8859-1\")[['v1','v2']]\n",
        "print(data.head())\n",
        "\n",
        "# Encode labels (spam=1, ham=0)\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(data['v1'])\n",
        "\n",
        "# Preprocessing function: remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    return \"\".join([c for c in text if c not in string.punctuation])\n",
        "\n",
        "data['clean_text'] = data['v2'].apply(remove_punctuation)\n",
        "\n",
        "# TF-IDF Features\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(data['clean_text'])\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Neural Network\n",
        "model = MLPClassifier(hidden_layer_sizes=100, max_iter=200, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# Test with new input\n",
        "new_text = [\"Ok lar... Joking wif u oni\"]\n",
        "y_new = vectorizer.transform(new_text)\n",
        "print(\"Prediction:\", model.predict(y_new))  # 0 = ham, 1 = spam\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS43O65D7EVd",
        "outputId": "cb385a0f-e5aa-47f2-bd48-60d72552b86c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     v1                                                 v2\n",
            "0   ham  Go until jurong point, crazy.. Available only ...\n",
            "1   ham                      Ok lar... Joking wif u oni...\n",
            "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3   ham  U dun say so early hor... U c already then say...\n",
            "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
            "Accuracy: 0.979372197309417\n",
            "Prediction: [0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "3uee7cdT-2vf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Project 4: Sentiment Analysis**"
      ],
      "metadata": {
        "id": "O6L3bFr8-3zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A. Using TextBlob**"
      ],
      "metadata": {
        "id": "2_rgy1Zf_F1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"The product quality exceeded my expectations and delivery was fast!\"\n",
        "blob = TextBlob(text)\n",
        "\n",
        "print(\"Polarity:\", blob.sentiment.polarity)\n",
        "if blob.sentiment.polarity > 0:\n",
        "    print(\"Positive\")\n",
        "elif blob.sentiment.polarity == 0:\n",
        "    print(\"Neutral\")\n",
        "else:\n",
        "    print(\"Negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXtuiz5U7ES_",
        "outputId": "58e1fcf6-dcb9-4baf-8e49-e177ce134056"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.25\n",
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **B. Using VADER**"
      ],
      "metadata": {
        "id": "K2SlPGle_QZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix7V9215_2so",
        "outputId": "088a5e71-77e8-45d5-985c-bd4ab7e19752"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vaderSentiment in /usr/local/lib/python3.12/dist-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "text = \"I am very disappointed with the poor customer support\"\n",
        "\n",
        "score = analyzer.polarity_scores(text)\n",
        "print(score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIfWSoxg7EMG",
        "outputId": "59e0a042-351f-43aa-8003-cd1d1a3fd44c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.427, 'neu': 0.395, 'pos': 0.178, 'compound': -0.5849}\n"
          ]
        }
      ]
    }
  ]
}